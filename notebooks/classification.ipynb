{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683f7b71",
   "metadata": {},
   "source": [
    "### Important Notes for Using This Notebook\n",
    "\n",
    "- Please set a device id\n",
    "- Due to data privacy and GitHub data size allowed you have to upload the data and word2vec pre-trained model yourself. <br>\n",
    "\tFor Turkish Word2vec pretrained model, you need to go to <a href=\"https://drive.google.com/drive/folders/1IBMTAGtZ4DakSCyAoA4j7Ch0Ft1aFoww\">this address</a> and download it\n",
    "- Please set file paths for checkpoint and model (checkpoint_file_path and model_name)\n",
    "- To be able to use different data independent of the dataset, you must load them yourself (for this go to 4.1 and you can see an example like reading text file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ece816",
   "metadata": {},
   "source": [
    "## 1. LOAD LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "source_path = (os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..'))+ '/src/')\n",
    "sys.path.append(source_path)\n",
    "\n",
    "# text preprocessing method\n",
    "from text_preprocessing import preprocessing\n",
    "\n",
    "gpu_number = DEVICE_ID #### SET SPECIAL DEVICE ID \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "\ttf.config.experimental.set_visible_devices(gpus[gpu_number], 'GPU') \n",
    "\tlogical_gpus = tf.config.experimental.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec919e1",
   "metadata": {},
   "source": [
    "## 2. Load Dataset and Word Vectors and Prepare Necessary objects for Modelling\n",
    "\n",
    "To download the Turkish Word2Vec Model https://drive.google.com/drive/folders/1IBMTAGtZ4DakSCyAoA4j7Ch0Ft1aFoww go to this link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36c0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "data_dir = os.path.join(main_dir, \"data/\")\n",
    "models_dir = os.path.join(main_dir, \"model/\")\n",
    "outputs_dir = models_dir = os.path.join(main_dir, \"outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_dir + \"your_dataset.csv\"\n",
    "word_vectors_path = data_dir + \"word_index.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee48a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word_vectors_path, binary=True)\n",
    "\n",
    "# we decided to remove two categories from the dataset that have less data than the others and we will solve them with regex\n",
    "df = df[~df.kategori.isin([\"Cumhurbaşkanlığı Kararnamesi\", \"Kanun Hükmünde Kararname\"])]\n",
    "\n",
    "# if no preprocessing has been done, use preprocessing method\n",
    "df.data_text = df.data_text.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c098799",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "ids = le.fit_transform(df.kategori)\n",
    "label_dict = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "\n",
    "pickle.dump(open(data_dir + \"label_dict.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66711708",
   "metadata": {},
   "source": [
    "<h4>INFO:</h4>\n",
    "<b>chunk_size</b> is a value to be used to augmentation data and it will use word_indexes to render the first N of the texts as a new line.\n",
    "\t\n",
    "            text = filter_by_word_index(\" \".join(text.split()[:chunk_size]))\n",
    "\n",
    "<b>max_length/max_len</b> limits a text to a specified value (0:N)\n",
    "\n",
    "            text = \" \".join(text.split()[:max_len])\n",
    "\n",
    "Note: if max_len < chunk_size, chunk_size is disabled       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614bfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values are random. If you want to see performance at different values, change these values as you want\n",
    "# When we evaluated the performances according max_len, best scores were observed at 64, 128 and 256 max_len\n",
    "CHUNK_SIZE = 300\n",
    "MAX_LEN = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db54a8",
   "metadata": {},
   "source": [
    "<i>DeepDataset module includes word_index, embedding_matrix, train, validation and test datasets for use in training</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DeepDataset\n",
    "dataset = DeepDataset(df, text_column=\"text\", label_column=\"kategori\", chunk_size=CHUNK_SIZE, word_vectors=word_vectors, max_len=MAX_LEN)\n",
    "dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ee0b2",
   "metadata": {},
   "source": [
    "#### Confusion Matrix Visualization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bd1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "\t\tplt.figure(figsize=(12,6))\n",
    "\t\tplt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\t\tplt.title(title)\n",
    "\t\tplt.colorbar()\n",
    "\t\ttick_marks = np.arange(len(classes))\n",
    "\t\tplt.xticks(tick_marks, classes, rotation=90)\n",
    "\t\tplt.yticks(tick_marks, classes)\n",
    "\n",
    "\t\tif normalize:\n",
    "\t\t\tcm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\t\t\tprint(\"Normalized confusion matrix\")\n",
    "\t\telse:\n",
    "\t\t\tprint('Confusion matrix, without normalization')\n",
    "\n",
    "\t\tthresh = cm.max() / 2.\n",
    "\t\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "\t\t\tplt.text(j, i, cm[i, j],\n",
    "\t\t\t\t\t\thorizontalalignment=\"center\",\n",
    "\t\t\t\t\t\tcolor=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.ylabel('True label')\n",
    "\t\tplt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c4708",
   "metadata": {},
   "source": [
    "## 3. TRAINING    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_STOPPING = True\n",
    "CLASS_WEIGHTS = True\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 256 # decrease this value if you have insufficient GPU capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelling import BiLSTM_Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# create callbacks\n",
    "# set file path to save for checkpoint\n",
    "checkpoint = ModelCheckpoint(models_dir + \"model_checkpoint.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "earlystopping = EarlyStopping(monitor=\"val_accuracy\", min_delta=0.001, patience=9, mode=\"max\")\n",
    "\n",
    "callbacks = [checkpoint, earlystopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d82a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model and start training\n",
    "model = BiLSTM_Model(epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=None)\n",
    "model.train(dataset, class_weights=CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.model.save(models_dir + \"model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b32c21",
   "metadata": {},
   "source": [
    "## 4. TESTING and RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6817c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict method is in modeling.py, not in tensorflow/keras and for tensorflow classic methods use model.model.predict\n",
    "\n",
    "# model.predict stores class probabilities and class values\n",
    "model.predict(dataset.test_data)\n",
    "\n",
    "predictions = model.predictions\n",
    "test_labels = dataset.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show performance scores\n",
    "print(metrics.classification_report(test_labels, predictions, target_names=list(dataset.label_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot confusion matrix\n",
    "cm = confusion_matrix(test_labels, predictions)\n",
    "plot_confusion_matrix(cm, classes=list(dataset.label_dict.keys()), title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b25851",
   "metadata": {},
   "source": [
    "#### 4.1. Predict Special Text OR Text List (From dataframe, json, file etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96ed9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, word_index, max_len, text_list):\n",
    "\n",
    "\ttokenized = []\n",
    "\tfor text in text_list:\n",
    "\t\tpadded_custom = [0 for i in range(max_len)]\n",
    "\t\t_ = [padded_custom.__setitem__(i, word_index[word]) for i, word in enumerate(text.split()[:max_len]) if word in word_index]\n",
    "\t\ttokenized.append(padded_custom)   \n",
    "\n",
    "\tclass_probabilities = model.predict(tokenized)\n",
    "\tprediction = np.argmax(class_probabilities, axis=1)\n",
    "\treturn prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = load_model(models_dir + \"model.hdf5\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5812f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing requires word index and tag dict loading\n",
    "word_index = pickle.load(data_dir + \"word_index.pkl\")\n",
    "label_dict = pickle.load(data_dir + \"label_dict.pkl\")\n",
    "\n",
    "# swap keys and values to convert predictions into categories\n",
    "reverse_label_dict = {v:k for k,v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload sample text from a text file and preprocessing\n",
    "with open(data_dir + \"sample.txt\", \"r\") as f:\n",
    "    sample_text = f.read()\n",
    "    sample_text = preprocessing(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use [text] pattern for single text\n",
    "# otherwise use text list\n",
    "# assign max_len value the model uses \n",
    "prediction = prediction(model, word_index, MAX_LEN, [sample_text])\n",
    "print(reverse_label_dict[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26abd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e5cdcf29e6ac6d41db7b25e6e646745a9c3c14d94f4e2d05336539cd6cf747b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
